# AlexNet

> 首次证明了，学习到的特征，可以超越手动涉及的特征，且优势明显。

##### 网络结构

> 5 + 3 : 5 个卷积层 + 3 个全连接层

- 5 个卷积层
  
  > 卷积核，逐渐变小 [11, 5, 3, 3, 3]
  > 
  > 通道数，先增加，后减少 [96, 256, 384, 384, 256]
  > 
  > 步幅，逐渐变小 [4, 2, 1, 1, 1]
  > 
  > [1, 2, 5] 卷积层后面，分别跟一个最大汇聚层，3x3，stride = 2

- 3 个全连接层
  
  > 维度逐渐变小 [4096, 4096, 1000]
  > 
  > 除了输出层外，全连接层后面，都跟了一层 dropout，用于降低模型容量，减少过拟合

- 非线性 / 激活函数
  
  > 卷积层 / 全连接层，都是线性变换。因此，每一层后面，都跟一个 ReLU ，提供非线性性。
  > 
  > 优势
  > 
  > 1. 整个正数定义域，都由梯度， 可以更新参数。
  > 
  > 2. 梯度简单，整数定义域为1， 负数定义域为0 

- 最大汇聚层
  
  > 3x3, 步幅为 2
  > 
  > 则特征图的形状变为
  > 
  > $$
  > h1 = \lceil (h-2)/2 \rceil
  > $$
  > 
  > $$
  > w1 = \lceil (w-2) / 2 \rceil
  > $$

##### 优化处理

- 数据增强
  
  > 翻转 / 裁剪 / 变色

- 使用 GPU 加速计算

##### 资源需求

- 参数个数
  
  > 全连接层远远多于卷积层
  > 
  > 每个输入元素，都由独立的权重，因此，权重参数的个数很多。
  > 
  > 卷积层，复用卷积核的参数，因此参数个数少很多。
  > 
  > 因此，全连接层，对内存的占用大很多。

- 浮点数运算次数
  
  > 卷积层，多于全连接层
  > 
  > 卷积层的运算次数，由 6 个因子，连乘决定。
  > 
  > 全连接层，运算次数和参数个数相同，仅仅有两个乘积因子。
  > 
  > 因此，卷积层，对计算资源消耗更多。

##### 参数个数计算

- 卷积层
  
  > $$
  > C_{out} * C_{in} * K_{h} * K_{w} + C_{out}
  > $$
  > 
  > 表示 w 的参数个数 + b 的参数个数

- 全连接层
  
  > $$
  > d_{in} * d_{out}
  > $$
  > 
  > 表示输入的元素个数 * 输出的元素个数

##### 浮点数运算次数计算

- 卷积层
  
  > $$
  > C_{out} * C_{in} * K_{h} * K_{w} * O_{h} * O_{w} + C_{out} * O_{h} * O_{w}
  > $$
  > 
  > 表示 w 的运算次数 +  b 的运算次数
  > 
  > 激活层 / 汇聚层的计算次数较少。
  > 
  > 激活层：
  > 
  > $$
  > C_{in} * I_{h} * I_{w}
  > $$
  > 
  > 汇聚层：
  > 
  > $$
  > C_{in} * K_{h} * K_{w} * O_{h} * O_{w}
  > $$

- 全连接层
  
  > $$
  > d_{in} * d_{out}
  > $$

参考：

[AlexNet, 动手学深度学习](https://zh.d2l.ai/chapter_convolutional-modern/alexnet.html)
