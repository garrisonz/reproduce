##### ResNet

##### 残差块, Residual Block

- 结构
  
  > 2个卷积层
  > 
  > 每个卷积层，都么都跟一个 BN 层，对卷积层的输出，做规范化。
  > 
  > 第一个卷积层后面，跟 BN 层 + ReLU 层
  > 
  > 第二个卷积层，BN 规范化后，和原始输入 X 做相加。
  > 
  > 再做一个 ReLU ，提供非线性
  > 
  > 其中：
  > 
  > - 若卷积层改变了特征图的通道数，或有 stride 改变了高 / 宽，则做相加之前，对 X 做 1x1 卷积，调整通道数 / 高 / 宽。使得相加两个 tensor 形状一样。
  > 
  > - 第一层卷积层，调整到目标输出通道数
  >   
  >   第二层卷积层，则输入 / 输出的通道数，都是目标输出通道数。

- 特征图的形状的改变
  
  > 残差块，通过传入的输出通道数和 `strides`，改变特征图的通道数 和 高 / 宽

##### 残差模块, resnet_block

- 模块结构
  
  > 仅仅非首模块的第一个残差块, Residual Block，对特征图做下采样，即高 / 宽 分别减半，通道数翻倍
  > 
  > 其他残差块，都不改变特征图的形状。
  > 
  > 其中，浅网络，例如 18-layer, 34-layer，第一个模块不改变通道数
  > 
  > 而深网络，例如 50-layer, 101-layer, 152-layer，第一个模块，就开始提升通道数。升至 256 个通道数。
  > 
  > ---
  > 
  > 深度网络，例如，50-layer, 101-layer, 152-layer
  > 
  > 都采用瓶颈模块结构
  > 
  > 即， 1x1 降低通道数，3x3 提取空间信息，1x1 再恢复通道数。

##### ResNet

- ResNet-18 网络结构
  
  > 第一个模块，7x7 卷积 + 最大汇聚，令通道数升至 64，高 / 宽两次减半，故高宽为原来的 1/4
  > 
  > 第 2 ~ 5 个模块，都是 resnet_block，分别包含 2 个残差块 Residual Block.
  > 
  > 第 2 个残差模块，特征图的形状不变
  > 
  > 第 3 ~ 5 残差模块，每个残差模块，通道数翻倍，高 / 宽减半。

- ResNet-18 参数个数
  
  > 约 11.5M 个
  > 
  > 计算过程： [ResNet_analysis.ipynb](https://github.com/garrisonz/reproduce/blob/main/ResNet/ResNet_analysis.ipynb)
